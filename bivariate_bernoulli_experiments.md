# Bivariate Bernoulli experiments

- work in progress
- relevant to A/B testing

---

## Experiment setup

Both experimental setups (i) and (ii) below yield the sample proportions:

- $\hat{p}_1 = \frac{1}{n_1} \sum_{i=1}^{n_1} y_{1,i} = \frac{n_{11}}{n_1}$
- $\hat{p}_2 = \frac{1}{n_2} \sum_{i=1}^{n_2} y_{2,i} = \frac{n_{21}}{n_2}$

Each is obtainable from the first order conditions of the maximised Bernoulli likelihood,

$$
\mathcal{L}(p_k)=\prod_{i=1}^{n_k}p_k^{y_{k,i}}(1-p_k)^{1-y_{k,i}}
$$

Where $y_k$ is the count of successes in $n_k$ trials in group $k$.

We can set up the contingency table in terms of counts:

|           | success | failure | total |
|-----------|---------|---------|-------|
| **group 1** | $n_{11}$ | $n_{12}$ | $n_1=n_{11}+n_{12}$ |
| **group 2** | $n_{21}$ | $n_{22}$ | $n_2=n_{21}+n_{22}$ |
| **total** | $n_{11}+n_{21}$ | $n_{12}+n_{22}$ | $N=n_{11}+n_{12}+n_{21}+n_{22}$ |

Then $p_1=\frac{n_{11}}{n_1}$ and $p_2=\frac{n_{21}}{n_2}$.

Or we can set up a general $2\times 2$ case:

- group indicator $X \in \{0,1\} \equiv \{\text{control}, \text{treatment}\}$
- outcome indicator $Y \in \{0,1\} \equiv \{\text{failure}, \text{success}\}$

We can also label *treatment as exposure* and *success as conversion* and so on.

| | success ($Y=1$) | failure ($Y=0$) | marginal $X$ |
|-|-----------------|-----------------|--------------|
| **treatment** ($X=1$) | $P(X = 1 \cap Y = 1)$<br>$=P(Y = 1 \mid X = 1) P(X = 1)$<br>$=p_1 \times P(X = 1)$ | $P(X = 1 \cap Y = 0)$<br>$=P(Y = 0 \mid X = 1) P(X = 1)$<br>$=(1-p_1) \times P(X = 1)$ | $P(X=1)$ |
| **control** ($X=0$) | $P(X = 0 \cap Y = 1)$<br>$=P(Y = 1 \mid X = 0) P(X = 0)$<br>$=p_2 \times P(X = 0)$ | $P(X = 0 \cap Y = 0)$<br>$=P(Y = 0 \mid X = 0) P(X = 0)$<br>$=(1-p_2) \times P(X = 0)$ | $P(X=0)$ |
| marginal $Y$ | $P(Y=1)$ | $P(Y=0)$ | |

This is a joint Bernoulli distribution governed by parameters $p_1=P(Y = 1 \mid X = 1)$ and $p_2=P(Y = 1 \mid X = 0)$. Our null is then $H_0: p_1 = p2$.

**Experiment (i)**: [testing Fisher's lady's taste](https://en.wikipedia.org/wiki/Lady_tasting_tea)

One participant undergoes $N$ trials. In $\frac{N}{2}$ trials, the single participant tastes $A$ and in the other half they taste $B$ and say whether they have tasted $A$ or $B$.

In this experiment, the unit of analysis is the trial. The $N$ trials are not independent but repeated (clustered).

This experiment requires a non-elementary approach owing to clustered data.

**Experiment (ii)**: testing Scout cubs' taste ([A/B test](https://en.wikipedia.org/wiki/A/B_testing))

$N$ participants undergo one trial each. In $\frac{N}{2}$ trials, the participants taste $A$ and in the other half participants taste $B$. Each participant says whether they have tasted $A$ or $B$.

In this experiment, the unit of analysis is the participant. The $N$ trials can claim to be independent.

**Experiment (iii)**: testing the relationship between smoking and cancer

*See* Agresti (2019), chapter 2. If a retrospective conditions on a design split into two groups unrepresentative of the background rate, the risk ratio is invalid. In other words, the orientation of the table matters.

## Bayesian estimation

Set up the model with, 

- $y_k \mid p_k \sim \text{binomial}(n_k, p_k), \text{for } k=1,2$
- $p_k \sim \text{Beta}(\alpha, \beta)$, choose $p_k \sim \text{Beta}(\frac{1}{2}, \frac{1}{2})$ (Jeffreys prior) 
- $p_k \mid y_k \sim \text{Beta}(y_k+\alpha,n_k-y_k+\beta) = \text{Beta}(\frac{1}{2}+y_k,\frac{1}{2}+n_k-y_k)$ (implied posterior)

Then,

$$
P(p_k \mid y_k) \propto P(y_k \mid p_k) P(p_k) = \left(p_k^{y_k}(1-p_k)^{n_k-y_k}\right)\left(p_k^{\frac{1}{2}-1}(1-p_k)^{\frac{1}{2}-1}\right) = p_k^{y_k-\frac{1}{2}}(1-p_k)^{n_k-y_k-\frac{1}{2}}
$$

And,

$$
\mathbb{E}[p_k \mid y_k]=\frac{\frac{1}{2}+y_k}{1+n_k}
$$

Generally, we model $g(\theta \mid x)=\frac{f(y \mid \theta) g(\theta)}{\int f(y \mid \theta) g(\theta) \, d\theta} \propto f(y \mid \theta) g(\theta)$. The Bernoulli and binomial distributions conjugate with Beta, hence the derivation above. $\theta \sim \text{Beta}(\alpha, \beta) \Rightarrow \mathbb{E}\left[\theta\right]=\frac{\alpha}{\alpha+\beta}$, but $\hat{\theta}$ can also be computed as the median of the posterior simulations. Their quantiles give the credible intervals.

---

## Toolbox

Effect measures

- risk difference
- risk ratio
- odds ratio

Significance testing

- z-score
- Wald
- Fisher's exact test
- $\chi^2$ test

---

## Effect measures

| measure | estimator | null | use case |
|---------|-----------|------|----------|
| risk difference | $\delta=p_1-p_2$ | $H_0:\delta=\delta_0=0$ | |
| risk ratio | $\rho=\frac{p_1}{p_2}$ | $H_0:\rho=\rho_0=1$ | |
| odds ratio | $\theta=\frac{p_1/(1-p_1)}{p_2(1-p_2)}$ | $H_0:\theta=\theta_0=1$ | |

Recall that frequentists speak from the perspective of random data generated by a true parameter, hence $P(y \mid \theta)$. Bayesians speak of uncertainty about the parameter and correspondingly model the parameter as a random variable, thus $P(\theta \mid y)$. Riks differences, risk ratios and odds ratios are interpreted accordingly.

The interpretation of confidence intervals and credible intervals follows. With $\alpha=0.05$, if the experiment were run an infinite number of times, *the confidence intervals would contain the true parameter* 95% of the time. But *the parameter falls within the credible bands* 95% of the time in the Bayesian framework. 

| measure | frequentist CI | Bayesian CrI | interpretation |
|---------|----------------|--------------|----------------|
| RD | Wald/Score/Newcombe | Posterior draws from $p_1 - p_2$ | absolute difference in risk |
| RR | delta method | Posterior draws from $p_1/p_2$   | relative risk |
| OR | Woolf/Cornfield | Posterior draws from logit ratio | odds ratio |

---

### Risk Difference (RD)

#### Frequentist CI ([the flawed Wald](https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Fss%2F1009213286))

$$
\hat \delta = \hat p_1 - \hat p_2,
$$

$$
\widehat{\mathrm{SE}}(\hat \delta)
= \sqrt{\frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2}}.
$$

**Wald CI:**

$$
\mathrm{RD} \in
\hat \delta \pm Z_{1-\alpha/2} \cdot \widehat{\mathrm{SE}}(\hat \delta).
$$

[**Miettinenâ€“Nurminen**](https://doi.org/10.1002/sim.4780040211) (1985) or [**Newcombe**](https://doi.org/10.1002/(SICI)1097-0258(19980430)17:8<873::AID-SIM779>3.0.CO;2-I) (1998) score-based intervals preferrable. Follow [CRAN references](https://search.r-project.org/CRAN/refmans/DescTools/html/BinomDiffCI.html).

#### Bayesian credible interval

Sample $\delta = p_1 - p_2$ (draw from both Betas, subtract, then take the 2.5 % and 97.5 % quantiles).

---

### Risk Ratio (RR)

#### Frequentist CI (delta method/log)

$$
\hat \rho = \frac{\hat p_1}{\hat p_2}, \qquad
\log \hat \rho = \log(\hat p_1) - \log(\hat p_2).
$$

$$
\widehat{\mathrm{SE}}(\log \hat \rho)
= \sqrt{\frac{1 - \hat p_1}{\hat p_1 n_1} + \frac{1 - \hat p_2}{\hat p_2 n_2}}=\sqrt{\frac{1}{n_{11}} - \frac{1}{n_{11}+n_{12}} + \frac{1}{n_{21}} - \frac{1}{n_{21}+n_{22}}}
$$

$$
\rho \in \exp{\left( \ln{\hat{\rho}}\pm Z_{\alpha/2}\sqrt{\widehat{\mathrm{SE}}(\ln{\hat{\rho}})} \right)}
$$

#### Bayesian credible interval

Draw $p_1$, $p_2$ from Beta posteriors as above, compute

$$
\rho = \frac{p_1}{p_2},
$$

and take posterior quantiles (e.g. 95 %).

---

### Odds Ratio (OR)

#### Frequentist CI ([**Woolf**](https://doi.org/10.1111/j.1469-1809.1955.tb01348.x) (1955))

$$
\hat \theta = \frac{n_{11} n_{22}}{n_{12} n_{21}}, \quad
\log \hat \theta = \log n_{11} + \log n_{22} - \log n_{12} - \log n_{21},
$$

$$
\widehat{\mathrm{SE}}(\ln{\hat{\theta}})
= \sqrt{\frac{1}{n_{11}}+\frac{1}{n_{12}}+\frac{1}{n_{21}}+\frac{1}{n_{22}}}.
$$

Note that we commonly add $0.5$ to each denominator to avoid division by zero.

$$
\theta \in \exp{\left( \ln{ \hat \theta \pm Z_{\alpha/2}\ \widehat{\mathrm{SE}}(\ln{\hat{\theta}}) } \right)}
$$

Alternatively, compute the **Fisher exact Cornfield interval** by inverting the hypergeometric distribution.

Refer to [**Gardner** and **Morris**](https://www.jstor.org/stable/29530653) (1988) and [**Szumilas**](https://pmc.ncbi.nlm.nih.gov/articles/PMC2938757/) (2010).

#### Bayesian credible interval

Place priors on $p_1$, $p_2$ or on $\log\theta$.

From posterior draws:

$$
\theta = \frac{p_1/(1 - p_1)}{p_2/(1 - p_2)}.
$$

The 2.5 % and 97.5 % quantiles give a 95 % credible interval.

---

## Significance testing

Assume $X_1 \sim \text{Bernoulli}(p_1)$ and $n_1$ and $X_2 \sim \text{Bernoulli}(p_2)$ and $n_2$.

A two sample test compares two alternatives.

- $H_0: p_1 = p_2 = p$ 
- $H_a: p_1 \neq p_2$

Then, the test statistic takes the form of a standardised score,

|  | z-score | Wald |
|--------------|---------|---------|
| statistic  | $z=\frac{\hat{p}_1-\hat{p}_2}{\sqrt{\hat{p}(1-\hat{p})\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}}$ where $\hat{p} = \frac{x_1 + x_2}{n_1 + n_2}$ | $W=\frac{(\hat{p}_1-\hat{p}_2)^2}{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}$ |
| distribution | $z \overset{d}{\to} \mathcal{N}(0,1)$ | $W \overset{d}{\to} \chi_{(1)}^2$ (asymptotically) |

Generalising beyond Bernoulli, $t=\frac{\hat{p}_1-\hat{p}_2}{\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}} \sim t_{N-1}$.

### Size ($\alpha$), power ($1-\beta$), effect size ($\delta$) and sample size ($n$)

The central limit theorem them tells us,

| under $H_0$ | under $H_a$ |
|-------------|-------------|
| $\hat{p}_1 \sim \mathcal{N}\left(p, \frac{p(1-p)}{n_1}\right)$ | $\hat{p}_1 \sim \mathcal{N}\left(p_1, \frac{p_1(1-p_1)}{n_1}\right)$ |
| $\hat{p}_2 \sim \mathcal{N}\left(p, \frac{p(1-p)}{n_2}\right)$ | $\hat{p}_2 \sim \mathcal{N}\left(p_2, \frac{p_2(1-p_2)}{n_1}\right)$ |
| $\hat{\delta}=\hat{p}_1-\hat{p}_2 \sim \mathcal{N}\left(0, p(1-p)\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\right)$ | $\hat{\delta}=\hat{p}_1-\hat{p}_2 \sim \mathcal{N}\left(\delta, \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}\right)$ |

Recall the confusion matrix.

|             | negative test (do not reject $H_0$) | positive test (reject $H_0$) | |
|-------------|------------------------------------|------------------------------|-|
| $H_0$ true  | true negative | false positive (Type I Error) | specificity $=\frac{TN}{TN+FP}$ |
| $H_0$ false | false negative (Type II Error) | true positive | sensitivity $=\frac{TP}{FN+TP}$ |
|             | negative predictive value $=\frac{TN}{TN+FN}$ | precision $=\frac{TP}{FP+TP}$ | accuracy $=\frac{TN+TP}{TN+FP+FN+TP}$ |

Then,

- true negative $\Rightarrow$ confidence level ($1-\alpha$)
- false positive $\Rightarrow$ significance level ($\alpha=P(\text{Type I Error})=\text{size}$)
- false negative $\Rightarrow \beta=P(\text{Type II Error})$
- true positive $\Rightarrow (1-\beta)=\text{power}$

Note that

- larger power ($1-\beta$) requires a larger sample size ($n$)
- larger size ($\alpha$) requires a larger sample size
- lower effect size ($\delta=|p_a-p_0|$) requires a larger sample size

Also recall that the following relationship yields the confidence intervals,

$$
P\left( -Z_{\alpha/2} \leq \frac{\hat{p}-p_0}{\sqrt{p_0(1-p_0)/n}} \leq Z_{\alpha/2} \right)=1-\alpha
$$

Assuming $n_1=n_2=n$ and solving for the confidence interval bounds,

$$
p_{\text{crit}}=\pm Z_{\alpha/2}\sqrt{p(1-p)/n}
$$

Setting the bound distance from $\delta$,

$$
\delta - Z_\beta \sqrt{p_1(1-p_1)/n+p_2(1-p_2)/n}=Z_{\alpha/2}\sqrt{2p(1-p)/n}
$$

Assuming $2p(1-p)=p_1(1-p_1) + p_2(1-p_2)$ and solving for $n$,

$$
n = \frac{ \left(p_1(1-p_1) + p_2(1-p_2) \right) (Z_{\alpha/2} + Z_\beta)^2 }{ \delta^2 }
$$

### Fisher exact test

Right-tailed p-value,

$$
p = \sum_{j=n_{11}}^{\min(n_1, n_{11}+n_{12})} \frac{\binom{n_1}{j} \binom{n_2}{(n_{11}+n_{12})-j}}{\binom{N}{n_{11}+n_{12}}}
$$

Two-tailed p-value,

$$
p = \sum_{\substack{j=0 \\ P(j) \leq P(n_{11})}}^{\min(n_1, n_{11}+n_{12})} \frac{\binom{n_1}{j} \binom{n_2}{(n_{11}+n_{12})-j}}{\binom{N}{n_{11}+n_{12}}}
$$

Where $P(n_{11})$ is the observed table and $P(j) =  \frac{\binom{n_1}{j} \binom{n_2}{(n_{11}+n_{12})-j}}{\binom{N}{n_{11}+n_{12}}}$.

Check the p-value against the following decision rule,

- $p \leq \alpha$: sufficient evidence against $H_0:$ evidence suggests a statistically significant association
- $p \geq \alpha$: insufficient evidence against $H_0$

### Pearson chi squared test

$$
\chi^2=\sum_{ij}\frac{(n_{ij}-\mu_{ij})^2}{\mu_{ij}}=\sum_{ij}\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\sim\chi_{(1)}^2
$$

Where $n_{ij}=O_{ij}$ are the observed $\mu_{ij}=E_{ij}$ the expected counts and $df=(rc-1)-[(r-1)+(c-1)]=(r-1)(c-1)$ (parameters under $H_a$ less parameters under $H_0$).

- $H_0: n_{ij}=\mu_{ij}$ (the variables are independent)
- $H_a: n_{ij}\neq\mu_{ij}$ (there is evidence of an association between at least two variables)

### Likelihood ratio statistic

$\chi^2$ and the likelihood ratio test the like hypothesis of independence (no association between variables) and, asymptotically, share the distribution.

$$
G^2=2 \sum_{ij} n_{ij} \ln{ \left( \frac{n_{ij}}{\mu_{ij}} \right) } = 2 \sum_{ij} O_{ij} \ln{ \left( \frac{O_{ij}}{E_{ij}} \right) }  \sim \chi_{(1)}^2
$$

---

## Statistics and p-values in R and Python

```R
# R
  # Two-tailed p-value
z <- 1.96
p <- 2 * (1 - pnorm(abs(z)))

  # Two-tailed z-score
p <- 0.05
z <- qnorm(1 - p / 2)
```

```Python
# Python
  # Two-tailed p-value
from scipy.stats import norm

z = 1.96
p = 2 * (1 - norm.cdf(abs(z)))

  # Two-tailed z-score
from scipy.stats import norm

p = 0.05
z = norm.ppf(1 - p / 2)
```

---

## Some references

Agresti, A. (2010) *Analysis of Ordinal Categorical Data*, 2nd edn. Wiley.

Agresti, A. (2019) *An Introduction to Categorical Data Analysis*, 3rd edn. Wiley.

Agresti, A. (2002) *Categorical Data Analysis*, 2nd edn. Wiley.

Brown, L.D., Cai, T.T. and DasGupta, A. (2001) 'Interval Estimation for a Binomial Proportion'. *Statistical Science*, Vol. 16, No. 2, 101-133. https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Fss%2F1009213286

Pither, J. (2025) 'Analyzing associations between two categorical variables'. *Tutorials for BIOL202: Introduction to Biostatistics*. https://ubco-biology.github.io/BIOL202/two_cat.html

Robert, D. (2020) 'Five Confidence Intervals for Proportions That You Should Know About'. *Towards Data Science*. https://towardsdatascience.com/five-confidence-intervals-for-proportions-that-you-should-know-about-7ff5484c024f/
