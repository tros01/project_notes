# Bivariate Bernoulli experiments

- work in progress
- relevant to A/B testing

---

## Experiment setup

Both experimental setups (i) and (ii) below yield the sample proportions $\hat{p}_1=\frac{1}{n_1}\sum_{i=1}^{n_1}y_{1,i}=\frac{a}{n_1}$ and $\hat{p}_2=\frac{1}{n_2}\sum_{i=1}^{n_2}y_{2,i}=\frac{c}{n_2}$. Each is obtainable from the first order conditions of the maximised Bernoulli likelihood $\mathcal{L}(p_k)=\prod_{i=1}^{n_k}p_k^{y_{k,i}}(1-p_k)^{1-y_{k,i}}$ where $y_k$ is the count of successes in $n_k$ trials in group $k$.

We can set up the contingency table in terms of counts:

|           | success | failure | total |
| --------- | ------- | ------- | ----- |
| **group 1**   | $a$     | $b$     | $n_1=a+b$ |
| **group 2**   | $c$     | $d$     | $n_2=c+d$ |
| **total** | $a+c$   | $b+d$   | $N=a+b+c+d$   |

Then $p_1=\frac{a}{n_1}$ and $p_2=\frac{c}{n_2}$.

Or we can set up a general $2\times 2$ case:

- group indicator $X \in \{0,1\}$ (control, exposure/treatment)
- outcome indicator $Y \in \{0,1\}$ (failure, success)

| | success ($Y=1$) | failure ($Y=0$) | marginal $X$ |
|-|-----------------|-----------------|--------------|
| **treatment** ($X=1$) | $P(X = 1 \cap Y = 1)$<br>$=P(Y = 1 \mid X = 1) P(X = 1)$<br>$=p_1 \times P(X = 1)$ | $P(X = 1 \cap Y = 0)$<br>$=P(Y = 0 \mid X = 1) P(X = 1)$<br>$=(1-p_1) \times P(X = 1)$ | $P(X=1)$ |
| **control** ($X=0$) | $P(X = 0 \cap Y = 1)$<br>$=P(Y = 1 \mid X = 0) P(X = 0)$<br>$=p_2 \times P(X = 0)$ | $P(X = 0 \cap Y = 0)$<br>$=P(Y = 0 \mid X = 0) P(X = 0)$<br>$=(1-p_2) \times P(X = 0)$ | $P(X=0)$ |
| marginal $Y$ | $P(Y=1)$ | $P(Y=0)$ | |

This is a joint Bernoulli distribution governed by parameters $p_1=P(Y = 1 \mid X = 1)$ and $p_2=P(Y = 1 \mid X = 0)$. Our null is then $H_0: p_1 = p2$.

**Experiment (i)**: [testing Fisher's lady's taste](https://en.wikipedia.org/wiki/Lady_tasting_tea)

One participant undergoes $N$ trials. In $\frac{N}{2}$ trials, the single participant tastes $A$ and in the other half they taste $B$ and say whether they have tasted $A$ or $B$.

In this experiment, the unit of analysis is the trial. The $N$ trials are not independent but repeated (clustered).

Note that clustered data require a different approach.

**Experiment (ii)**: testing Scout cubs' taste ([A/B test](https://en.wikipedia.org/wiki/A/B_testing))

$N$ participants undergo one trial each. In $\frac{N}{2}$ trials, the participants taste $A$ and in the other half participants taste $B$. Each participant says whether they have tasted $A$ or $B$.

In this experiment, the unit of analysis is the participant. The $N$ trials can claim to be independent.

## Bayesian estimation

Set up the model with $y_k \mid p_k \sim \text{binomial}(n_k, p_k)$, $p_k \sim \text{Beta}(\frac{1}{2}, \frac{1}{2})$ (Jeffreys prior) and the implied posterior $p_k \mid y_k \sim \text{Beta}(\frac{1}{2}+y_k,\frac{1}{2}+n_k-y_k)$ for $k=1,2$ (our groups and outcomes). Then,

$$
P(p_k \mid y_k) \propto P(y_k \mid p_k) P(p_k) = \left(p_k^{y_k}(1-p_k)^{n_k-y_k}\right)\left(p_k^{\frac{1}{2}-1}(1-p_k)^{\frac{1}{2}-1}\right) = p_k^{y_k-\frac{1}{2}}(1-p_k)^{n_k-y_k-\frac{1}{2}}
$$

And,

$$
\mathbb{E}[p_k \mid y_k]=\frac{\frac{1}{2}+y_k}{1+n_k}
$$

Generally, we model $g(\theta \mid x)=\frac{f(y \mid \theta) g(\theta)}{\int f(y \mid \theta) g(\theta) \, d\theta} \propto f(y \mid \theta) g(\theta)$. The Bernoulli and binomial distributions conjugate with Beta, hence the derivation above. $\theta \sim \text{Beta}(\alpha, \beta) \Rightarrow \mathbb{E}\left[\theta\right]=\frac{\alpha}{\alpha+\beta}$, but $\hat{\theta}$ can also be computed as the median of the posterior simulations. Their quantiles give the credible intervals.

---

## Toolbox

Effect measures

- risk difference
- risk ratio
- odds ratio

Significance testing

- z-score
- Wald
- Fisher's exact test
- $\chi^2$ test

---

## Effect measures

| measure | estimator | null | use case |
|---------|-----------|------|----------|
| risk difference | $\delta=p_1-p_2$       | $H_0:\delta=\delta_0$        | |
| risk ratio      | $\rho=\frac{p_1}{p_2}$ | $H_0:\frac{p_1}{p_2}=\rho_0$ | |
| odds ratio      | $\theta=\frac{p_1/(1-p_1)}{p_2(1-p_2)}$ | $H_0:\theta=1$ | |

Recall that frequentists speak from the perspective of random data generated by a true parameter, hence $P(y \mid \theta)$. Bayesians speak of uncertainty about the parameter and correspondingly model the parameter as a random variable, thus $P(\theta \mid y)$. Riks differences, risk ratios and odds ratios are interpreted accordingly.

The interpretation of confidence intervals and credible intervals follows. With $\alpha=0.05$, if the experiment were run an infinite number of times, *the confidence intervals would contain the true parameter* 95% of the time. But *the parameter falls within the credible bands* 95% of the time in the Bayesian framework. 

| measure | frequentist CI                             | Bayesian CrI                     | interpretation             |
| ------- | ------------------------------------------ | -------------------------------- | --------------------------- |
| RD      | Wald/Score/Newcombe                        | Posterior draws from (p_1 - p_2) | absolute difference in risk |
| RR      | log-transformed                            | Posterior draws from (p_1/p_2)   | relative risk               |
| OR      | log-transformed (Woolf or Cornfield exact) | Posterior draws from logit ratio | odds ratio                  |

---

### Risk Difference (RD)

#### Frequentist CI ([the flawed Wald](https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Fss%2F1009213286))

$$
\hat \delta = \hat p_1 - \hat p_2,
$$

$$
\widehat{\mathrm{SE}}(\hat \delta)
= \sqrt{\frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2}}.
$$

**Wald CI:**

$$
\mathrm{RD} \in
\hat \delta \pm Z_{1-\alpha/2} \cdot \widehat{\mathrm{SE}}(\hat \delta).
$$

[**Miettinenâ€“Nurminen**](https://doi.org/10.1002/sim.4780040211) (1985) or [**Newcombe**](https://doi.org/10.1002/(SICI)1097-0258(19980430)17:8<873::AID-SIM779>3.0.CO;2-I) (1998) score-based intervals preferrable. Follow [CRAN references](https://search.r-project.org/CRAN/refmans/DescTools/html/BinomDiffCI.html).

#### Bayesian credible interval

Sample $\delta = p_1 - p_2$ (draw from both Betas, subtract, then take the 2.5 % and 97.5 % quantiles).

---

### Risk Ratio (RR)

#### Frequentist CI (delta method/log)

$$
\hat \rho = \frac{\hat p_1}{\hat p_2}, \qquad
\log \hat \rho = \log(\hat p_1) - \log(\hat p_2).
$$

$$
\widehat{\mathrm{SE}}(\log \hat \rho)
= \sqrt{\frac{1 - \hat p_1}{\hat p_1 n_1} + \frac{1 - \hat p_2}{\hat p_2 n_2}}=\sqrt{\frac{1}{a} - \frac{1}{a+b} + \frac{1}{c} - \frac{1}{c+d}}
$$

$$
\rho \in \exp{\left( \ln{\hat{\rho}}\pm Z_{\alpha/2}\sqrt{\widehat{\mathrm{SE}}(\ln{\hat{\rho}})} \right)}
$$

#### Bayesian credible interval

Draw $p_1$, $p_2$ from Beta posteriors as above, compute

$$
\rho = \frac{p_1}{p_2},
$$

and take posterior quantiles (e.g. 95 %).

---

### Odds Ratio (OR)

#### Frequentist CI ([**Woolf**](https://doi.org/10.1111/j.1469-1809.1955.tb01348.x) (1955))

$$
\hat \theta = \frac{a d}{b c}, \quad
\log \hat \theta = \log a + \log d - \log b - \log c,
$$

$$
\widehat{\mathrm{SE}}(\ln{\hat{\theta}})
= \sqrt{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d}}.
$$

Note that we commonly add $0.5$ to each denominator to avoid division by zero.

$$
\theta \in \exp{\left( \ln{ \hat \theta \pm Z_{1-\alpha/2}\ \widehat{\mathrm{SE}}(\ln{\hat{\theta}}) } \right)}
$$

Alternatively, compute the **Fisher exact Cornfield interval** by inverting the hypergeometric distribution.

Refer to [**Gardner** and **Morris**](https://www.jstor.org/stable/29530653) (1988) and [**Szumilas**](https://pmc.ncbi.nlm.nih.gov/articles/PMC2938757/) (2010).

#### Bayesian credible interval

Place priors on $p_1$, $p_2$ or on $\log\theta$.

From posterior draws:

$$
\theta = \frac{p_1/(1 - p_1)}{p_2/(1 - p_2)}.
$$

The 2.5 % and 97.5 % quantiles give a 95 % credible interval.

---

## Significance testing

Assume $X_1 \sim \text{Bernoulli}(p_1)$ and $n_1$ and $X_2 \sim \text{Bernoulli}(p_2)$ and $n_2$.

A two sample test compares two alternatives.

- $H_0: p_1 = p_2 = p$ 
- $H_a: p_1 \neq p_2$

Then, the test statistic takes the form of a standardised score,

|  | z-score | Wald |
|--------------|---------|---------|
| statistic  | $z=\frac{\hat{p}_1-\hat{p}_2}{\sqrt{\hat{p}(1-\hat{p})\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}}$ where $\hat{p} = \frac{x_1 + x_2}{n_1 + n_2}$ | $W=\frac{(\hat{p}_1-\hat{p}_2)^2}{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}$ |
| distribution | $z \overset{d}{\to} \mathcal{N}(0,1)$ | $W \overset{d}{\to} \chi_{(1)}^2$ (asymptotically) |

Generalising beyond Bernoulli, $t=\frac{\hat{p}_1-\hat{p}_2}{\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+\frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}} \sim t_{N-1}$.

### Size ($\alpha$), power ($1-\beta$), effect size ($\delta$) and sample size ($n$)

The central limit theorem them tells us,

| under $H_0$ | under $H_a$ |
|-------------|-------------|
| $\hat{p}_1 \sim \mathcal{N}\left(p, \frac{p(1-p)}{n_1}\right)$ | $\hat{p}_1 \sim \mathcal{N}\left(p_1, \frac{p_1(1-p_1)}{n_1}\right)$ |
| $\hat{p}_2 \sim \mathcal{N}\left(p, \frac{p(1-p)}{n_2}\right)$ | $\hat{p}_2 \sim \mathcal{N}\left(p_2, \frac{p_2(1-p_2)}{n_1}\right)$ |
| $\hat{\delta}=\hat{p}_1-\hat{p}_2 \sim \mathcal{N}\left(0, p(1-p)\left(\frac{1}{n_1}+\frac{1}{n_2}\right)\right)$ | $\hat{\delta}=\hat{p}_1-\hat{p}_2 \sim \mathcal{N}\left(\delta, \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}\right)$ |

Recall the confusion matrix.

|             | negative test (do not reject $H_0$) | positive test (reject $H_0$) |
|-------------|------------------------------------|------------------------------|
| $H_0$ true  | true negative | false positive (Type I Error) |
| $H_0$ false | false negative (Type II Error) | true positive |

Then,

- true negative $\Rightarrow$ confidence level ($1-\alpha$)
- false positive $\Rightarrow$ significance level ($\alpha=P(\text{Type I Error})=\text{size}$)
- false negative $\Rightarrow \beta=P(\text{Type II Error})$
- true positive $\Rightarrow (1-\beta)=\text{power}$

Note that

- larger power ($1-\beta$) requires a larger sample size ($n$)
- larger size ($\alpha$) requires a larger sample size
- lower effect size ($\delta=|p_a-p_0|$) requires a larger sample size

Also recall that the following relationship yields the confidence intervals,

$$
P\left( -Z_{\alpha/2} \leq \frac{\hat{p}-p_0}{\sqrt{p_0(1-p_0)/n}} \leq Z_{\alpha/2} \right)=1-\alpha
$$

Assuming $n_1=n_2=n$ and solving for the confidence interval bounds,

$$
p_{\text{crit}}=\pm Z_{\alpha/2}\sqrt{p(1-p)/n}
$$

Setting the bound distance from $\delta$,

$$
\delta - Z_\beta \sqrt{p_1(1-p_1)/n+p_2(1-p_2)/n}=Z_{\alpha/2}\sqrt{2p(1-p)/n}
$$

Assuming $2p(1-p)=p_1(1-p_1) + p_2(1-p_2)$ and solving for $n$,

$$
n = \frac{ \left(p_1(1-p_1) + p_2(1-p_2) \right) (Z_{\alpha/2} + Z_\beta)^2 }{ \delta^2 }
$$

### Fisher exact test

Right-tailed p-value,

$$
p = \sum_{j=a}^{\min(n_1, a+c)} \frac{\binom{n_1}{j} \binom{n_2}{(a+c)-j}}{\binom{N}{a+c}}
$$

Two-tailed p-value,

$$
p = \sum_{\substack{j=0 \\ P(j) \leq P(a)}}^{\min(n_1, a+c)} \frac{\binom{n_1}{j} \binom{n_2}{(a+c)-j}}{\binom{N}{a+c}}
$$

Where $P(a)$ is the observed table and $P(j) =  \frac{\binom{n_1}{j} \binom{n_2}{(a+c)-j}}{\binom{N}{a+c}}$.

Check the p-value against the following decision rule,

- $p \leq \alpha$: sufficient evidence against $H_0:$ evidence suggests a statistically significant association
- $p \geq \alpha$: insufficient evidence against $H_0$

### Pearson chi squared test

$$
\chi^2=\sum_{ij}\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\sim\chi_{(1)}^2
$$

Where $O_{ij}$ are the observed $E_{ij}$ the expected counts.

- $H_0:$ the variables are independent
- $H_a:$ there is an association between at least two variables

---

## Statistics and p-values in R and Python

```R
# R
  # Two-tailed p-value
z <- 1.96
p <- 2 * (1 - pnorm(abs(z)))

  # Two-tailed z-score
p <- 0.05
z <- qnorm(1 - p / 2)
```

```Python
# Python
  # Two-tailed p-value
from scipy.stats import norm

z = 1.96
p = 2 * (1 - norm.cdf(abs(z)))

  # Two-tailed z-score
from scipy.stats import norm

p = 0.05
z = norm.ppf(1 - p / 2)
```

---

## Some references

Agresti, A. (2019) *An Introduction to Categorical Data Analysis*, 3rd edn. Wiley.

Agresti, A. (2002) *Categorical Data Analysis*, 2nd edn. Wiley.

Brown, L.D., Cai, T.T. and DasGupta, A. (2001) 'Interval Estimation for a Binomial Proportion'. *Statistical Science*, Vol. 16, No. 2, 101-133. https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Fss%2F1009213286

Pither, J. (2025) 'Analyzing associations between two categorical variables'. *Tutorials for BIOL202: Introduction to Biostatistics*. https://ubco-biology.github.io/BIOL202/two_cat.html

Robert, D. (2020) 'Five Confidence Intervals for Proportions That You Should Know About'. *Towards Data Science*. https://towardsdatascience.com/five-confidence-intervals-for-proportions-that-you-should-know-about-7ff5484c024f/
